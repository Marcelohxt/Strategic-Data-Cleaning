# !pip install -q plotly numpy pandas watermark missingno category_encoders xlrd 



#!pip install openpyxl


# Versão da Linguagem Python
from platform import python_version
print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())


#import math  # Biblioteca padrão do Python para funções matemáticas (ex: sqrt, sin, cos, log)

#import sys, os  # 'sys' fornece acesso a variáveis do sistema e funções do interpretador Python
                # 'os' permite interagir com o sistema operacional (ex: manipular arquivos e diretórios)

#import numpy as np  # NumPy: Biblioteca para cálculos numéricos e manipulação de arrays multidimensionais

#import pandas as pd  # Pandas: Biblioteca para manipulação e análise de dados, especialmente tabelas (DataFrames)


# Imports
import math
import sys, os
import numpy as np
import pandas as pd
import openpyxl 



# Vamos adicionar caminho para os módulos Python

sys.path.append(os.path.abspath(os.path.join('modulos')))
from estrategia1 import * # o Asteriscos importa tudo que estra dentro no Modulo! 
from estrategia2 import * # o Asteriscos importa tudo que estra dentro no Modulo! 
from estrategia3 import * # o Asteriscos importa tudo que estra dentro no Modulo! 
from openpyxl import load_workbook

# aqui é um modulo ja configurado em Pthyton com 3 estrategias de tratamento de dados, que podemos usalos em outros projetos e tambem complemente-los 
# Isso ira facilitar nosso processo e acelerar mais nosso processo de analise! 


# Configura o Pandas para exibir até 100 colunas ao imprimir um DataFrame.
# Útil quando trabalhamos com tabelas grandes, evitando que colunas fiquem ocultas com "...".

pd.set_option('display.max_columns',100) 





# Criamos uma lista para identificar valores ausentes
lista_labels_valores_ausentes = ["n/a", "na", "undefined"]


# Carrega o dataset
# Carrega o arquivo CSV localizado em "dados/dataset.csv" em um DataFrame chamado 'dataset'.
# Durante a leitura do arquivo, os valores especificados em 'lista_labels_valores_ausentes' 
# serão tratados como valores ausentes (NaN). Isso permite que o pandas reconheça e trate 
# adequadamente valores como 'NA', 'null', ou 'indisponível' como ausentes no DataFrame.
dataset = pd.read_csv("dados/dataset.csv", na_values = lista_labels_valores_ausentes)


# Shape
dataset.shape


# amostra de Dados 

dataset.head()


dicionario = pd.read_excel("dados/Dicionario.xlsx")


dicionario.shape


dicionario.head()





dataset.info()


dataset.describe() # ele calcula as estatisca para nos DETALHE ELE SO OLHA PARA AS COLUNAS INT OU FLOAT OU SEJA NUMERICAS





dataset.shape


dicionario.shape





# Concatena duas séries de dados lado a lado (axis=1) para comparar as colunas do dataset com os campos do dicionário.
# A primeira série contém a lista de nomes das colunas do DataFrame 'dataset', obtida com dataset.columns.tolist().
# A segunda série contém os valores da coluna 'Fields' do DataFrame 'dicionario', que provavelmente tem os nomes dos campos esperados.
# O resultado é armazenado em 'df_compara_colunas', que terá duas colunas: uma com os nomes das colunas do dataset e outra com os campos do dicionário.
df_compara_colunas = pd.concat([pd.Series(dataset.columns.tolist()), 
                                dicionario['Fields']], 
                               axis = 1)


# Renomeia as colunas do DataFrame 'df_compara_colunas' para melhorar a legibilidade.
# O método 'rename' é utilizado para alterar os nomes das colunas.
# O parâmetro 'columns' recebe um dicionário onde:
#   - A chave '0' se refere à primeira coluna do DataFrame, que originalmente tem índice 0 e será renomeada para 'Coluna no Dataset'.
#   - A chave 'Fields' se refere à coluna 'Fields' do DataFrame, que será renomeada para 'Coluna no Dicionario'.
# O parâmetro 'inplace=True' garante que a renomeação seja feita diretamente no DataFrame original, sem a necessidade de criar uma nova variável.
df_compara_colunas.rename(columns = {0: 'Coluna no Dataset', 'Fields': 'Coluna no Dicionario'}, 
                          inplace = True)


df_compara_colunas





# Romenando Colunas 
# Renomeia as colunas do DataFrame 'dataset' para melhorar a legibilidade e ajustar os nomes das colunas.
# O método 'rename' é usado para alterar os nomes das colunas de acordo com o mapeamento fornecido no dicionário.
# O parâmetro 'columns' recebe um dicionário onde:
#   - 'Dur. (ms)' será renomeado para 'Dur (s)', alterando a unidade de milissegundos para segundos.
#   - 'Dur. (ms).1' será renomeado para 'Dur (ms)', ajustando o nome para refletir corretamente os milissegundos.
#   - 'Star ms' será renomeado para 'Start Offset (ms)', clarificando o termo 'início' e a unidade de milissegundos.
#   - 'End ms' será renomeado para 'End offset (ms)', esclarecendo que se trata do 'final' com a unidade de milissegundos.
# O parâmetro 'inplace=True' aplica as mudanças diretamente no DataFrame original, sem necessidade de criar uma nova variável.

dataset.rename(columns = { 'Dur. (ms)' : 'Dur (s)' ,
                          'Dur. (ms).1' : 'Dur (ms)',
                          'Star ms' : 'Start Offset (ms)',
                          'End ms' : 'End offset (ms)'},
               inplace = True)


# Lista de colunas do Dataset

dataset.columns.tolist()





help(func_calc_percentual_valores_ausentes)


func_calc_percentual_valores_ausentes(dataset)


df_missing = func_calc_percentual_valores_ausentes_coluna(dataset)


df_missing





# Seleciona as colunas que possuem 30% ou mais de valores ausentes
# df_missing['% de Valores Ausentes'] >= 30.00 -> Filtra apenas as colunas com pelo menos 30% de valores ausentes
# .index -> Obtém os nomes dessas colunas (índices do DataFrame)
# .tolist() -> Converte os nomes das colunas em uma lista

colunas_para_remover = df_missing[df_missing['% de Valores Ausentes'] >= 30.00].index.tolist()


colunas_para_remover





# Colunas que serão removidas 

# Cria uma nova lista chamada 'colunas_para_remover' que contém apenas as colunas
# da lista original 'colunas_para_remover' que **não** estão presentes na lista
# ['TCP UL Retrans. Vol (Bytes)', 'TCP DL Retrans. Vol (Bytes)'].
# Em outras palavras, essa linha remove da lista 'colunas_para_remover' as colunas
# 'TCP UL Retrans. Vol (Bytes)' e 'TCP DL Retrans. Vol (Bytes)', caso elas existam.


colunas_para_remover = [col for col in colunas_para_remover if col not in ['TCP UL Retrans. Vol (Bytes)',
    'TCP DL Retrans. Vol (Bytes)']]


colunas_para_remover


# Remove as colunas especificadas na lista 'colunas_para_remover' do DataFrame 'dataset'
# O parâmetro 'axis=1' indica que a remoção ocorre nas colunas (e não nas linhas)

dataset_clean = dataset.drop(colunas_para_remover, axis = 1)


dataset_clean.shape





func_calc_percentual_valores_ausentes(dataset_clean)


func_calc_percentual_valores_ausentes_coluna(dataset_clean)








fix_missing_bfill(dataset_clean , 'TCP UL Retrans. Vol (Bytes)')


fix_missing_bfill(dataset_clean , 'TCP DL Retrans. Vol (Bytes)')





dataset_clean['Avg RTT DL (ms)'].skew(skipna = True)


dataset_clean['Avg RTT UL (ms)'].skew(skipna = True)








fix_missing_ffill(dataset_clean , 'Avg RTT DL (ms)')


fix_missing_ffill(dataset_clean , 'Avg RTT UL (ms)')





func_calc_percentual_valores_ausentes(dataset_clean)


func_calc_percentual_valores_ausentes_linha(dataset_clean)


func_calc_percentual_valores_ausentes_coluna(dataset_clean)


dataset_clean.info()








# imputação de variaveis categoricas 
fix_missing_value(dataset_clean, 'Handset Type', 'unknown')
fix_missing_value(dataset_clean, 'Handset Manufacturer', 'unknown')





func_calc_percentual_valores_ausentes(dataset_clean)


func_calc_percentual_valores_ausentes_linha(dataset_clean)





# Drop de linhas com valores ausentes
drop_rows_with_missing_values(dataset_clean)


func_calc_percentual_valores_ausentes(dataset_clean)


dataset_clean.shape





dataset_clean.dtypes


dataset_clean





#Converte colunas específicas para o tipo string
convert_to_datetime(dataset_clean, ['Start' , 'End'])


# Seleciona todas as colunas do dataset que são do tipo 'object' (strings/categorias) 
# e as transforma em uma lista
string_columns = dataset_clean.select_dtypes(include = 'object').columns.tolist() 


columns = ['Last Location Name','Handset Manufacturer','Handset Type']


string_columns


convert_to_string(dataset_clean, string_columns)





#Lista de Colunas para conversão

int_cols = ['Bearer Id', 'IMSI', 'MSISDN/Number', 'IMEI',]


convert_to_int(dataset_clean, int_cols)


dataset_clean.dtypes


# Vamos checar se há registros duplicados
drop_duplicates(dataset_clean)





# conversao e compação 
temp_df = dataset_clean[['Dur (s)' , 'Dur (ms)']].copy()
multiply_by_factor(temp_df, ['Dur (ms)'], 1/1000)
temp_df['comparison'] = (temp_df['Dur (s)'] == temp_df['Dur (ms)'].apply(math.floor))


temp_df


#As duas colunas são iguais? 
print(all(temp_df['comparison']))





# Drop de coluna
drop_columns(dataset_clean, ['Dur (s)'])








# Cria o objeto trata outlier
trata_outlier = TrataOutlier(dataset_clean)


# Lista de colunas float64
lista_colunas = dataset_clean.select_dtypes('float64').columns.tolist()


lista_colunas


# Visão geral dos outliers
trata_outlier.getOverview(lista_colunas)


# Replace dos outliers
trata_outlier.replace_outliers_with_fences(lista_colunas)


# Visão geral dos outliers
trata_outlier.getOverview(lista_colunas)








dataset_clean['Social Media Data Volume (Bytes)'] = dataset_clean['Social Media UL (Bytes)'] + dataset_clean['Social Media DL (Bytes)']


dataset_clean['Google Data Volume (Bytes)'] = dataset_clean['Google UL (Bytes)'] + dataset_clean['Google DL (Bytes)']


dataset_clean['Email Data Volume (Bytes)'] = dataset_clean['Email UL (Bytes)'] + dataset_clean['Email DL (Bytes)']


dataset_clean['Youtube Data Volume (Bytes)'] = dataset_clean['Youtube UL (Bytes)'] + dataset_clean['Youtube DL (Bytes)']


dataset_clean['Netflix Data Volume (Bytes)'] = dataset_clean['Netflix UL (Bytes)'] + dataset_clean['Netflix DL (Bytes)']


dataset_clean['Gaming Data Volume (Bytes)'] = dataset_clean['Gaming UL (Bytes)'] + dataset_clean['Gaming DL (Bytes)']


dataset_clean['Other Data Volume (Bytes)'] = dataset_clean['Other UL (Bytes)'] + dataset_clean['Other DL (Bytes)']


dataset_clean['Total Data Volume (Bytes)'] = dataset_clean['Total UL (Bytes)'] + dataset_clean['Total DL (Bytes)']


dataset_clean.info()






